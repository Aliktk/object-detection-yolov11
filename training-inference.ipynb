{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the necessary libraries\n",
    "`!pip install opencv-python ultralytics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov11s.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write a function to predict and detect objects in images and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(chosen_model, img, classes=[], conf=0.5):\n",
    "    if classes:\n",
    "        results = chosen_model.predict(img, classes=classes, conf=conf)\n",
    "    else:\n",
    "        results = chosen_model.predict(img, conf=conf)\n",
    "\n",
    "    return results\n",
    "\n",
    "def predict_and_detect(chosen_model, img, classes=[], conf=0.5, rectangle_thickness=2, text_thickness=1):\n",
    "    results = predict(chosen_model, img, classes, conf=conf)\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            cv2.rectangle(img, (int(box.xyxy[0][0]), int(box.xyxy[0][1])),\n",
    "                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])), (255, 0, 0), rectangle_thickness)\n",
    "            cv2.putText(img, f\"{result.names[int(box.cls[0])]}\",\n",
    "                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), text_thickness)\n",
    "    return img, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict()` function\n",
    "\n",
    "This function takes three arguments:\n",
    "\n",
    "`chosen_model`: The trained model to use for prediction\n",
    "\n",
    "`img`: The image to make a prediction on\n",
    "\n",
    "`classes`: (Optional) A list of class names to filter predictions to\n",
    "\n",
    "`conf`: (Optional) The minimum confidence threshold for a prediction to be considered\n",
    "\n",
    "The function first checks if the `classes` argument is provided. If it is, then the `chosen_model.predict()` method is called with the classes argument, which filters the predictions to only those classes. Otherwise, the chosen_model.`predict()` method is called without the `classes` argument, which returns all predictions.\n",
    "\n",
    "The `conf` argument is used to filter out predictions with a confidence score lower than the specified threshold. This is useful for removing false positives.\n",
    "\n",
    "The function returns a list of prediction results, where each result contains the following information:\n",
    "\n",
    "* `name`: The name of the predicted class\n",
    "* `conf`: The confidence score of the prediction\n",
    "* `box`: The bounding box of the predicted object\n",
    "* `predict_and_detect()` function\n",
    "\n",
    "This function takes the same arguments as the `predict()` function, but it also returns the annotated image in addition to the prediction results.\n",
    "\n",
    "The function first calls the `predict()` function to get the prediction results. Then, it iterates over the prediction results and draws a bounding box around each predicted object. The name of the predicted class is also written above the bounding box.\n",
    "\n",
    "The function returns a tuple containing the annotated image and the prediction results.\n",
    "\n",
    "Here is a summary of the differences between the two functions:\n",
    "\n",
    "The `predict()` function only returns the prediction results, while the `predict_and_detect()` function also returns the annotated image.\n",
    "The `predict_and_detect()` function is a wrapper around the predict() function, which means that it calls the `predict()` function internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Detecting Objects in Images with YOLOv11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 11 persons, 2 cars, 3 traffic lights, 1 handbag, 561.7ms\n",
      "Speed: 14.8ms preprocess, 561.7ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# read the image\n",
    "image = cv2.imread(\"image.png\")\n",
    "result_img, _ = predict_and_detect(model, image, classes=[], conf=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save and Plot the result Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"Image\", result_img)\n",
    "cv2.imwrite(\"result.jpg\", result_img)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Detecting Objects in Videos with YOLOv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r\"YourVideoPath\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    result_img, _ = predict_and_detect(model, img, classes=[], conf=0.5)\n",
    "    cv2.imshow(\"Image\", result_img)\n",
    "    \n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save the result Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function for creating a writer (for mp4 videos)\n",
    "def create_video_writer(video_cap, output_filename):\n",
    "    # grab the width, height, and fps of the frames in the video stream.\n",
    "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "    # initialize the FourCC and a video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    writer = cv2.VideoWriter(output_filename, fourcc, fps,\n",
    "                             (frame_width, frame_height))\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"YourFilename\"\n",
    "\n",
    "video_path = r\"YourVideoPath\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "writer = create_video_writer(cap, output_filename)\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    result_img, _ = predict_and_detect(model, img, classes=[], conf=0.5)\n",
    "    writer.write(result_img)\n",
    "    cv2.imshow(\"Image\", result_img)\n",
    "    \n",
    "    cv2.waitKey(1)\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I train a YOLO11 model for object detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "media",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
